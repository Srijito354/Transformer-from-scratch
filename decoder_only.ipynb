{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook got no new idea from my side. A blunt code-along of whatever I learnt from StatQuest's decoder only tutorial in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'need': 1, 'ya': 2}\n"
     ]
    }
   ],
   "source": [
    "#def embedding_gen(sentence):\n",
    "sentence = \"I need ya ya\"\n",
    "word_idx = {word:i for i, word in enumerate(sorted(set(sentence.split())))}\n",
    "print(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {'what': 0,\n",
    "                'is': 1,\n",
    "                'statquest': 2,\n",
    "                'awesome': 3,\n",
    "                '<EOS>': 4}\n",
    "\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[token_to_id[\"what\"], # First input we wanna process\n",
    "                        token_to_id[\"is\"],\n",
    "                        token_to_id[\"statquest\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"]],\n",
    "                        \n",
    "                        [token_to_id[\"statquest\"], # Second input we wanna process\n",
    "                        token_to_id[\"is\"],\n",
    "                        token_to_id[\"what\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"]]])\n",
    "\n",
    "labels = torch.tensor([[token_to_id[\"is\"],\n",
    "                        token_to_id[\"statquest\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"],\n",
    "                        token_to_id[\"<EOS>\"]],\n",
    "                        \n",
    "                        [token_to_id[\"is\"],\n",
    "                        token_to_id[\"what\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"],\n",
    "                        token_to_id[\"<EOS>\"]]])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_gen(word_idx):\n",
    "    global embedding_dim, vocab_size\n",
    "    embedding_dim = 16\n",
    "    vocab_size = len(word_idx)\n",
    "    embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "    global e_l\n",
    "    e_l = []\n",
    "    for i in range(vocab_size):\n",
    "        e_l.append(i)\n",
    "    e = torch.LongTensor(e_l)\n",
    "    e = embeddings(e)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encodings(vocab_size, embedding_dim):\n",
    "    pe = torch.zeros(vocab_size, embedding_dim)\n",
    "    for pos in range(vocab_size):\n",
    "        for i in range(0, 4, 2):\n",
    "            em1 = torch.sin(torch.tensor(pos/(10000**((2*i)/embedding_dim))))\n",
    "            em2 = torch.cos(torch.tensor(pos/(10000**((2*(i+1))/embedding_dim))))\n",
    "\n",
    "            pe[pos][i] = em1\n",
    "            pe[pos][i] = em2\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positional_encodings(nn.Module):\n",
    "    def __init__(self, d_model = embedding_dim, max_length = len(token_to_id)):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        for pos in range(max_length):\n",
    "            for i in range(0, 4, 2):\n",
    "                em1 = torch.sin(torch.tensor(pos/(10000**((2*i)/d_model))))\n",
    "                em2 = torch.cos(torch.tensor(pos/(10000**((2*(i+1))/d_model))))\n",
    "\n",
    "                pe[pos][i] = em1\n",
    "                pe[pos][i] = em2\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        return word_embeddings + self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the self-attention block; or reffered to as just the attention block as in the tutorial.\n",
    "Extra point: We can later use this class to create multi-head attention blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_q = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.w_k = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.w_v = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        f_q = self.w_q(q)\n",
    "        f_k = self.w_k(k)\n",
    "        f_v = self.w_v(v)\n",
    "\n",
    "        sims = torch.matmul(f_q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        scaled_sims = sims/torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "        attention_percents = softmax(scaled_sims, dim=self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, f_v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens=4, d_model=embedding_dim, max_length=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "        self.pe = positional_encodings(d_model)\n",
    "\n",
    "        self.self_attention = Attention(d_model=d_model)\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        word_embeddings = self.we(token_ids)\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        mask = torch.tril(torch.ones((torch_ids.size(dim=0)), (torch.size(dim=0))))\n",
    "        mask = mask == 0\n",
    "\n",
    "        self_attention_values = self.self_attention(position_encoded,\n",
    "                                                    position_encoded,\n",
    "                                                    position_encoded,\n",
    "                                                    mask=mask)\n",
    "\n",
    "        res_con_val = position_encoded + self_attention_values\n",
    "\n",
    "        fc_layer_output = self.fc_layer(res_con_val)\n",
    "\n",
    "        return fc_layer_output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tokens, labels = batch\n",
    "        output = self.forward(input_tokens[0])\n",
    "        loss = self.loss(output, labels[0])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=embedding_dim, max_len=6)\n",
    "\n",
    "model_input = torch.tensor([[token_to_id[\"what\"], # First input we wanna process\n",
    "                        token_to_id[\"is\"],\n",
    "                        token_to_id[\"statquest\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"]]]\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "predictions = model(model_input)\n",
    "predicted_id = torch.tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(110.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ceil(torch.tensor(221/2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
